# -*- coding: utf-8 -*-
"""regulatory_summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13CMjrs2SVEkU18S3pIpd2Dl870_qNnfY
"""

!pip install -q transformers torch sentencepiece PyMuPDF pdfminer.six nltk gradio

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

import torch, sys
print("Python:", sys.version.splitlines()[0])
print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    try:
        print("GPU name:", torch.cuda.get_device_name(0))
    except Exception as e:
        print("Couldn't fetch GPU name:", e)

from google.colab import files
uploaded = files.upload()
print("Uploaded:", list(uploaded.keys()))

import fitz


pdf_path = "/content/1686821361_RBI_-_DLG_Guidelines_(June_8,_2023).pdf"


doc = fitz.open(pdf_path)
full_text = ""

for page in doc:
    text = page.get_text()
    full_text += text + "\n"

print("Total characters extracted:", len(full_text))
print("\n--- Preview of first 1000 characters ---\n")
print(full_text[:1000])

import re

clean_text = full_text

clean_text = re.sub(r'\n+', ' ', clean_text)
clean_text = re.sub(r'\s+', ' ', clean_text)


clean_text = re.sub(r'http\S+|www\.\S+', '', clean_text)


clean_text = re.sub(r'RBI/\d{4}-\d{2}/\d+', '', clean_text)


clean_text = clean_text.strip()

print("Characters after cleaning:", len(clean_text))
print("\n--- Preview of cleaned text ---\n")
print(clean_text[:1000])

!pip install transformers sentencepiece --quiet

from transformers import pipeline


summarizer = pipeline("summarization", model="facebook/bart-large-cnn")


def chunk_text(text, max_tokens=900):
    sentences = text.split('. ')
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len((current_chunk + sentence).split()) <= max_tokens:
            current_chunk += sentence + ". "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

chunks = chunk_text(clean_text)


summaries = []
for i, chunk in enumerate(chunks):
    print(f"\n--- Summarizing chunk {i+1}/{len(chunks)} ---")
    summary = summarizer(chunk, max_length=200, min_length=50, do_sample=False)[0]['summary_text']
    summaries.append(summary)


final_summary = " ".join(summaries)

print("\n===== FINAL SUMMARY =====\n")
print(final_summary)

!pip install transformers sentencepiece --quiet

from transformers import pipeline


summarizer = pipeline(
    "summarization",
    model="sshleifer/distilbart-cnn-12-6",
    device=-1
)


def chunk_text(text, max_words=400):
    words = text.split()
    return [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

chunks = chunk_text(clean_text)

summaries = []
for i, chunk in enumerate(chunks):
    print(f"\n--- Summarizing chunk {i+1}/{len(chunks)} ---")
    summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']
    summaries.append(summary)

final_summary = " ".join(summaries)

print("\n===== FINAL SUMMARY =====\n")
print(final_summary)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

!pip install -q scikit-learn

from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('punkt')


from nltk.tokenize import sent_tokenize
sentences = sent_tokenize(clean_text)


vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)
tfidf = vectorizer.fit_transform(sentences)

import numpy as np
scores = tfidf.sum(axis=1).A1

N = 8
top_idx = np.argsort(scores)[-N:][::-1]
top_sentences = [sentences[i] for i in top_idx]

print("Top sentences (raw):\n")
for s in top_sentences:
    print("-", s)
print("\n")


from transformers import pipeline

summ = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6", device=-1)

combined = " ".join(top_sentences)

polished = summ(combined, max_length=180, min_length=40, do_sample=False)[0]['summary_text']

bullets = [b.strip() for b in polished.split('. ') if b.strip()]

final_bullets = bullets[:6]

print("Final bullets:\n")
for i, b in enumerate(final_bullets, 1):

    if not b.endswith('.'):
        b = b + '.'
    print(f"{i}. {b}")

from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=-1
)

from transformers import pipeline
import nltk
import PyPDF2

nltk.download('punkt')
nltk.download('punkt_tab')

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=-1)

pdf_path = "1686821361_RBI_-_DLG_Guidelines_(June_8,_2023).pdf"
pdf_reader = PyPDF2.PdfReader(open(pdf_path, "rb"))
raw_text = ""
for page in pdf_reader.pages:
    raw_text += page.extract_text()

from nltk.tokenize import sent_tokenize
sentences = sent_tokenize(raw_text)

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(sentences)
scores = np.asarray(X.sum(axis=1)).ravel()
sentence_scores = {sentences[i]: scores[i] for i in range(len(sentences))}
top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:7]

summary_text = summarizer(raw_text[:3000], max_length=150, min_length=50, do_sample=False)[0]['summary_text']

print("Top Sentences from TF-IDF:")
for sent in top_sentences:
    print("-", sent)

print("\nBART Model Summary:\n", summary_text)

from transformers import pipeline
import nltk
import fitz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from google.colab import files

nltk.download("punkt")
nltk.download("punkt_tab")

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=-1)

uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]
pdf_document = fitz.open(pdf_path)
raw_text = ""
for page in pdf_document:
    raw_text += page.get_text()

sentences = nltk.sent_tokenize(raw_text)
vectorizer = TfidfVectorizer()
sentence_vectors = vectorizer.fit_transform(sentences)
similarity_matrix = cosine_similarity(sentence_vectors)
scores = similarity_matrix.sum(axis=1)
sentence_scores = {sent: score for sent, score in zip(sentences, scores)}
top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:7]

summary_text = summarizer(raw_text[:3000], max_length=150, min_length=50, do_sample=False)[0]['summary_text']
print("Extractive Summary:\n", " ".join(top_sentences))
print("\nAbstractive Summary:\n", summary_text)

!pip install transformers pypdf2 nltk torch

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from PyPDF2 import PdfReader
from transformers import pipeline
import torch
from nltk.tokenize import sent_tokenize

device = -1

pdf_path = "1686821361_RBI_-_DLG_Guidelines_(June_8,_2023).pdf"
reader = PdfReader(pdf_path)
raw_text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])

sentences = sent_tokenize(raw_text)
sentence_scores = {sentence: len(sentence.split()) for sentence in sentences}
top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:7]

summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=device)
summary_text = summarizer(raw_text[:3000], max_length=150, min_length=50, do_sample=False)[0]['summary_text']

print("\n" + "="*80)
print("# ðŸ“„ RBI DLG Guidelines Summary (Clean & Professional)")
print("="*80 + "\n")
print("## ðŸ“ Extracted Key Sentences:\n")
for i, sentence in enumerate(top_sentences, 1):
    print(f"{i}. {sentence}")
print("\n## ðŸ“Œ Model-Generated Summary:\n")
print(summary_text)
print("\n" + "="*80)